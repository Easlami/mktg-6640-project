---
title: "XG Boost"
author: "Riley Walburger"
date: "2025-06-10"
output: html_document
---


# Setup 

## Load Packages 
```{r}

warning(suppressMessages(library(tidyverse))) 
library(quanteda)
library(janitor)
library(here)
library(skimr)

```
## Load Data
```{r}

raw_reviews <- read_csv(here("C:/Users/walbr/Desktop/U of U/3- Summer Semester 2025/Text Analysis/Group Project/Restaurant_Data.csv")) %>% 
  clean_names()

raw_reviews$style[is.na(raw_reviews$style)] <- "Missing"
raw_reviews$price[is.na(raw_reviews$price)] <- "Missing"  # or another method

# Create factor vars 
raw_reviews <- raw_reviews %>% 
  mutate(across(c(style, price), factor))

```


# EDA 

```{r}
# See data types and structure 
glimpse(raw_reviews)

summary(raw_reviews)
# Looks like we have a variety of reviews across many different styles of restaurants. 
# The most common type of restaurant is Korean with 2 variations of American restaurants following up. 


# Check data quality 
skim(raw_reviews)
# We are looking at a really clean dataset, there are hardly any issues with missing data and no duplicates

```

## Initial Text Analytics 

We decided to look into the some of the korean restaurants based on the EDA after seeing that there were a good sample group of them in the top 25 restaurants. 

Our New Goal is to 
- examine the restaurants in the top 25 and see what we can learn from reviews about how they are doing things better than their competitors
- Do a case study on a chosen restaurant or 2. We have a few options here 1 that is much lower on the ratings and another that is a middle of the pack restaurant. 
- We hope to provide insights to these restaurants as to how they can improve their restaurant experience to get higher ratings/get ranked higher by yelp

## filter to our subset of Korean Restaurants 
```{r}

# See the korean restaurants in the top 25 ranks 

# Low restaurants
raw_reviews %>% 
  filter(str_detect(style, "Korean")) %>% 
  filter(restaurant_name != "The Barn Cafe & Restaurant") %>% 
  group_by(restaurant_name) %>% 
  summarise(
    count = n(),
    star_rating = mean(star_rating, na.rm = TRUE),
    rank = mean(rank, na.rm = TRUE),
    number_of_reviews = mean(number_of_reviews)
  ) %>% 
  filter(rank > 150) %>% 
  arrange(rank) %>% 
  print() 

# High rated restaurants
top_korean_spots <- raw_reviews %>% 
  filter(str_detect(style, "Korean"),
         rank < 26)

# Low rated Korean Restaurants
bad_korean_spots <- raw_reviews %>% 
  filter(restaurant_name != "The Barn Cafe & Restaurant") %>% 
  filter(str_detect(style, "Korean"),
         rank > 150) 
```

## Positive Version 
```{r}
# Define custom words to remove
custom_stopwords <- c(stopwords("english"), "restaurant", "food", "great", "good", "place")

# Preprocessing
top_reviews_corpus <- corpus(top_korean_spots$comment)
top_reviews_tokens <- tokens(top_reviews_corpus, 
                             remove_punct = TRUE, 
                             remove_numbers = TRUE) %>%
  tokens_remove(custom_stopwords)

# Create a document-term matrix
top_reviews_dtm <- dfm(top_reviews_tokens)

# Inspect dimensions and preview a few rows of the DFM
dim(top_reviews_dtm)
head(top_reviews_dtm, n = 5)

library(quanteda.textstats)
tstat1 <- textstat_frequency(top_reviews_dtm)
head(tstat1, 10)

# Load textplots and visualize word frequencies as a word cloud
library(quanteda.textplots)
textplot_wordcloud(
  top_reviews_dtm,
  min_size = 0.5,
  max_size = 4,
  min_count = 3,
  max_words = 200,
  color = "darkblue",
  font = NULL,
  adjust = 0,
  rotation = 0.1,
  random_order = FALSE,
  random_color = FALSE,
  ordered_color = FALSE,
  labelcolor = "gray20",
  labelsize = 1.5,
  labeloffset = 0,
  fixed_aspect = TRUE,
  comparison = FALSE
)
```


## Combined Version 
```{r}
# Combine with group labels
combined_reviews <- bind_rows(
    bad_korean_spots %>% mutate(group = "Low Ranked"), 
    top_korean_spots %>% mutate(group = "Top Ranked")
)

# Create corpus
reviews_corpus <- corpus(combined_reviews, text_field = "comment")


tokens_reviews <- tokens(reviews_corpus,
                         what = "word",
                         remove_punct = TRUE,
                         remove_numbers = TRUE,
                         remove_symbols = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(custom_stopwords) %>%
  # tokens_wordstem(language = "english") %>%
  tokens_keep(min_nchar = 3) %>%
  tokens_ngrams(n = 1:2)

# Create a DFM grouped by sentiment
dfm_grouped <- dfm(tokens_reviews) %>%
  dfm_group(groups = docvars(reviews_corpus, "group"))

# Calculate TF-IDF
tfidf_df <- dfm_tfidf(dfm_grouped)

# Convert TF-IDF to a data.frame for interpretation
tfidf_df <- convert(tfidf_df, to="data.frame")


# Plot wordcloud
p <- textplot_wordcloud(
  dfm_grouped,
  comparison = TRUE,
  min_size = 0.5,
  max_size = 3,
  min_count = 5,
  max_words = 200,
  color = c("red", "darkgreen"),
  font = NULL,
  adjust = 0,
  rotation = 0.1,
  random_order = FALSE,
  random_color = FALSE,
  ordered_color = FALSE,
  labelcolor = "gray20",
  labelsize = 1.5,
  labeloffset = 0,
) 


```


## XgBoost 

```{r}
library(xgboost)
library(Matrix)
library(quanteda)
library(tidyverse)
library(dplyr)
library(stringr)
```

### Create DFM, Dense and Sparse DFMs
```{r}
corpus <- reviews_corpus
tokens <- tokens(corpus, remove_punct = TRUE)
dfm <- dfm(tokens, tolower = TRUE)
dense_dfm <- as.matrix(dfm) # Convert dfm to dense matrix
sparse_dfm <- Matrix(dense_dfm, sparse = TRUE) # Convert dense matrix to sparse matrix
```


```{r}
dummies <- model.matrix(~ style + price - 1, data = combined_reviews)
sparse_dummies <- as(dummies, "dgCMatrix")
```

```{r}
combined_data <- cbind(sparse_dummies, sparse_dfm)
```

```{r}
# Convert to numeric class labels
target <- as.numeric(as.factor(combined_reviews$group)) - 1  # xgboost expects 0-based classes  # Subtract 1 to start at 0

```

```{r}
target <- as.numeric(as.factor(combined_reviews$group)) - 1
unique(target)
length(unique(target))
```

## Model Creation
```{r}
library(xgboost)
library(caret)  # For data split

# Split into training and testing
set.seed(123)
train_idx <- createDataPartition(target, p = 0.8, list = FALSE)

dtrain <- xgb.DMatrix(data = combined_data[train_idx, ], label = target[train_idx])
dtest  <- xgb.DMatrix(data = combined_data[-train_idx, ], label = target[-train_idx])

# Define parameters
num_class <- length(unique(target))

params <- list(
  objective = "binary:logistic",
  eval_metric = "logloss"
)

# Train model
model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  watchlist = list(train = dtrain, eval = dtest),
  early_stopping_rounds = 10,
  verbose = 1
)
```

```{r}
pred_probs <- predict(model, dtest)
pred_labels <- if (num_class > 2) max.col(matrix(pred_probs, ncol = num_class, byrow = TRUE)) - 1 else as.numeric(pred_probs > 0.5)

confusionMatrix(factor(pred_labels), factor(target[-train_idx]))
```

```{r}
# Combine column names from dummies and dfm
text_feature_names <- colnames(dfm)
dummy_feature_names <- colnames(sparse_dummies)
all_feature_names <- c(dummy_feature_names, text_feature_names)
```

```{r}
importance_matrix <- xgb.importance(feature_names = all_feature_names, model = model)

xgb.plot.importance(importance_matrix, top_n = 20)
```

```{r}
# Don't convert to numeric target â€” use original factor
combined_reviews$group <- factor(combined_reviews$group, levels = c("group_low", "group_high"))
```


```{r}
library(dplyr)
library(tidyr)
library(quanteda)
library(quanteda.textplots)  # optional for visualizations
library(quanteda.textstats)

# Add target to the reviews dataset
combined_reviews$target <- target

# Create tokens and remove punctuation and stop words
tokens_clean <- tokens(reviews_corpus, remove_punct = TRUE) %>%
  tokens_remove(stopwords("en"))

# Create dfm from cleaned tokens
dfm_clean <- dfm(tokens_clean, tolower = TRUE)
# Convert dfm to a matrix and then to data.frame
dfm_mat <- as.data.frame(as.matrix(dfm_clean))

dfm_mat$group <- combined_reviews$target

# Make sure row count matches
stopifnot(nrow(dfm_mat) == nrow(combined_reviews))

# Bind group label
dfm_mat$group <- combined_reviews$target

# Split into two datasets for group 0 and group 1
group_0 <- dfm_mat[dfm_mat$group == 0, -ncol(dfm_mat)]
group_1 <- dfm_mat[dfm_mat$group == 1, -ncol(dfm_mat)]

# Sum word counts in each group
group_0_sums <- colSums(group_0)
group_1_sums <- colSums(group_1)

# Combine into a data frame
word_comparison <- data.frame(
  word = names(group_0_sums),
  group_0 = group_0_sums,
  group_1 = group_1_sums
)

# Calculate difference
word_comparison <- word_comparison %>%
  mutate(
    diff = group_1 - group_0,
    total = group_0 + group_1
  ) %>%
  arrange(desc(abs(diff)))

word_comparison <- word_comparison %>%
  rename(
    group_low = group_0,
    group_high = group_1
  )
# View most distinguishing words
head(word_comparison, 50)
```

## Adding a few More Columns 

```{r}
library(tidytext)
library(dplyr)

# Make sure the column exists
stopifnot("comment" %in% colnames(combined_reviews))

# Tokenize words from the 'comment' column
word_usage <- combined_reviews %>%
  mutate(doc_id = row_number()) %>%
  select(restaurant_name, comment) %>%
  unnest_tokens(word, comment, token = "words") %>%
  distinct(restaurant_name, word)

# Count how many restaurants used each word
word_restaurant_counts <- word_usage %>%
  count(word, name = "restaurant_count")
```

```{r}
# Convert importance matrix to data frame if it's not already
importance_df <- as.data.frame(importance_matrix)

# Rename column for easier joining
importance_df <- importance_df %>%
  rename(word = Feature, xgb_importance = Gain)
```

```{r}
word_comparison <- word_comparison %>%
  left_join(word_restaurant_counts, by = "word") %>%
  left_join(importance_df, by = "word") %>%
  mutate(
    restaurant_count = replace_na(restaurant_count, 0),
    xgb_importance = replace_na(xgb_importance, 0)
  )
```

```{r}
# View final table with new columns
head(word_comparison %>%
       select(word, group_low, group_high, diff, restaurant_count, xgb_importance) %>%
       arrange(desc(abs(diff))), 50)
```

```{r}
top_words <- word_comparison %>%
  arrange(desc(xgb_importance)) %>%
  slice_head(n = 20)

head(top_words, 20)
```

```{r}
library(officer)
library(flextable)

# Select relevant columns and arrange by importance
table_to_export <- word_comparison %>%
  select(word, group_low, group_high, diff, restaurant_count, xgb_importance) %>%
  arrange(desc(xgb_importance)) %>%
  slice_head(n = 50)  # top 50 words for presentation

# Create flextable object
ft <- flextable(table_to_export)

# Format table nicely (auto width, bold headers, etc.)
ft <- autofit(ft)
ft <- theme_vanilla(ft)

# Create Word document and add table
doc <- read_docx()
doc <- body_add_flextable(doc, ft)

# Save the Word doc
print(doc, target = "word_comparison_table.docx")
```

